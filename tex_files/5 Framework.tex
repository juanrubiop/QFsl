\chapter{The Framework} \label{chap:framework}
What follows is our proposal for how an FSL framework for QNLP would work.

The motivation behind this framework is the word embeddings themselves. As mentioned in \ref{sec:embeddings}, the methods to obtain word embeddings usually exploit the relationship words have with each other. This structure is apparent when computing basic measures with them. Indeed, the structure that predominates is their meaning, that is what informs how the vectors are oriented in their vector space, how far apart words are, how linearly combining them can produce other words, and how their linear combination gives us a clue about the meaning decomposition. This structure is not lost in DisCoCat. The tensor product of the word strings and the linear mapping both conserve but transform it and this structure is responsible for the final form the meaning vector of a sentence takes. An FSL approach based on transfer learning that wants to make use of the abundant classical embeddings should make use of this structure since, regardless of the system type it runs on, the definition of the sentence meaning space is the same, and we could change the word space to be of type \textbf{fHilb} instead of \textbf{FVec} with little under-the-hood tinkering required. This would make the quantum state and the meaning vectors live in the same linear structure. So first, we propose that an FSL framework for QNLP should find a way to use classical embeddings to inform a variational circuit about its parameters.

So the FSL circuit should first involve an encoding which takes into account the classical embeddings. We are still left with learning the sentence space specific to the task. So the FSL circuit should then involve a variational layer that is responsible for learning the task and learning how the sentence meaning space uses the word structure.

It is apparent that by studying the necessities of our specific case, we have arrived at a variational structure similar to that described in Liu et al's paper in section \ref{sec:who}.

The general framework for encoding a word is then the following. For every word, first, a variational layer whose parameters are informed by a classical embedding of the same word is applied. This is the PQE layer and is specific for each word. That, is, the initial quantum state of a word with embedding vector $\overrightarrow{word1}$ and a word with embedding vector $\overrightarrow{word2}$ are:
\begin{align}\label{eq:PQE}
    |\psi_1\rangle&=U(\{ \boldsymbol{\theta}_i \})|0\rangle \\
    |\psi_2\rangle&=U(\{ \boldsymbol{\varphi}_j \})|0\rangle,
\end{align}
respectively. Where $|\psi_1\rangle=|\psi_2\rangle$ and $\{\boldsymbol{\theta}_i\}=\{ \boldsymbol{\varphi}_i \} \iff \overrightarrow{word1}=\overrightarrow{word2}$. This ensures that the mapping is unique. The mapping should also be deterministic. There can be a variety of ways to choose a mapping $A:\overrightarrow{word}\rightarrow \{\boldsymbol{\theta}_i\}$ and it does not necessarily have to be linear. This mapping should be informed mostly by the structure from the embedding set that is wished to be preserved (e.g. the inner product among word embeddings). This layer is fixed and does not change through the training epochs. The mapping $A$ should be calculated in a classical system.

The next step is to construct a variational layer after the PQE that will be in charge of learning the task. The key step for this layer is that for words of the same pregroup type, the unitary applied to the quantum state should be the same. That is for two words $\overrightarrow{word1}$ and $\overrightarrow{word2}$, their complete encoding should be:
\begin{align*}
    |\Psi_1\rangle&=W(\{ \boldsymbol{\alpha_i} \})|\psi_1\rangle=W(\{ \boldsymbol{\alpha_i} \})U(\{ \boldsymbol{\theta}_i \})|0\rangle \\
    |\Psi_2\rangle&=W(\{ \boldsymbol{\beta_j} \})|\psi_2\rangle=W(\{ \boldsymbol{\beta_j} \})U(\{ \boldsymbol{\varphi}_j\})|0\rangle,
\end{align*}
where $W(\{\boldsymbol{\alpha}_i\})=W(\{ \boldsymbol{\beta}_i \}) \iff P.type(\overrightarrow{word1})=P.type(\overrightarrow{word2})$. That the same $W$ is used for all words of the same pregroup type means that the model can properly learn how the structure of the words affects the model output, instead of the PQE being just a costly state initialisation state.

A few considerations exist for choosing $W$. It does not need to be that the variational circuit for $W$ should be the same for all pregroup types. One can choose one ansatz for Nouns and another for Sentences, but it is easier to allow for the same base ansatz in all cases (although further research could be done to assert whether this comes with a trade-off in performance). The expressibility should also be considered. This was not mentioned for the PQE layer because the structure worth preserving usually leaves in a lower dimensional space than the $2^N$ dimension Hilbert space for $N$ qubits. The inner product (or cosine similarity) is just a single number for two embeddings regardless of the number of qubits or vector dimensions. Even if the PQE were severely limited in expressibility, the higher-dimensional manifold could admit a lower-dimensional projection of the structure-preserving solution. Furthermore, the output of the PQE is just another quantum state. Since it is fixed for all epochs, it does not add nor subtract from the expressibility of $W$. The $W$ layer should be responsible for finding a solution and so is more susceptible to the trappings of expressibility. The PQE would affect the solution by affecting the subset of the search space available to $W$, but this would be mitigated by choosing a relatively highly expressible ansatz. In $W$ is also where practical considerations like qubit architecture should be considered.

Figure \ref{circ:fslex} shows what a circuit implementing this framework would look like. This circuit is for a transitive sentence, where we remember that if a noun has type $N$, we map it to a two-qubit state, and the transitive verb has type $n(n^rsn^l)n$ and it is mapped to a five-qubit state. It is readily apparent how both words of type $N$ share a common variational layer $W$, different than that for the transitive verb. It is also apparent how even though two words of the same type $N$ appear in the sentence, both have a different PQE parametrization. The quantum state which encodes the meaning of the sentences is the middle wire, and we only consider only those output states where the measured qubits gave a measurement of 0. 

\input{circuits/fslqex}

The process to train a circuit using this framework would be the following:
\begin{enumerate}
    \item Define both a classical embedding set and a structure which will be preserved in the quantum circuit.
    \item Define the ansatz scheme that will be used for both the PQE and the variational layers, and for each pregroup type.
    \item Construct a deterministic mapping that will take as input the classical embedding of a word and will injectively output a set of parameters for the PQE layer.
    \item Run the training cycle as normal, updating the shared parameters in $W$ according to a cost function, evaluating the circuits accordingly, and sharing the processing among quantum and classical systems.
\end{enumerate}
After training a circuit, the deterministic mapping should take in a word that the model has not seen before and output a parametrization that, when run through the model, gives an output that is coherent with the rest of the words seen during training.