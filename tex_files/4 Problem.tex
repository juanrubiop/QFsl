\chapter{The Project} \label{chap:Problem}

The story so far tells us of a situation in which a method to perform useful NLP tasks on quantum computers exists, and if the NISQ era were far past it would be one that is as native to quantum systems as the simulations of quantum systems are. So DisCoCat on quantum systems is reduced to the world of variational algorithms that are costly to run and, as Shor's factorization, not useful until we can make use of bigger systems that can process vast amounts of data.

The first thought of many would be something along the lines of: "Hey, there is a direct translation between word meaning states and quantum states so why don't we train everything classically and just get the quantum state to be that of the word embedding". Indeed, in a perfect world that would be the answer, but NISQ systems strike again and for a unitary on $N$ qubits we would need around $4^N$ gates, so you say maybe we can use fewer gates and train the parameters, and then we have come full circle to the state of the art with HQCs and variational algorithms.

This reasoning is not necessarily all for nothing. During the review of the literature, we came across two interesting papers which showed some interesting possibilities \cite{nakaji_approximate_2022,mitsuda_approximate_2023}. Approximate Amplitude Encoding (AAE) as a training scheme is discussed in more depth in appendix \ref{app:aae}. It shows a method to variationally train circuits to encode in the amplitude of a quantum system some data. This idea suffers the same problems we will try to solve (which will be further explained) in this work but is an interesting path for further research.

Currently, methods for DisCoCat involve getting all of the sentences with all of the words that are going to be seen and training them on some task. For any given circuit in the testing dataset, the words it encode will already have been seen during training. OOV words cannot be inputted into the system because it will output incorrect results. This presents some obvious problems. We would have to know beforehand all of the words we want the system to learn. This is unfeasible because we cannot have perfect knowledge of the needs of the future. This will also mean training on a bigger data set, so more computation time, more resources, and more costs. And if we had a system that we would want to upgrade and add more vocabulary to, it would need to be retrained. This same problem plagues the implementation of AAE.

But what if we not only could train a circuit knowing it would work with words it hasn't seen yet, but also we could train it with a subset of the words we are aware it needs to know? This would mean that we would have found some way in which to lessen the system requirements for parameter learning and provide more flexibility to develop more complex algorithms for QNLP tasks

Given this goal and the restrictions imposed by the NISQ era, this work aims to develop an FSL framework for QNLP that will reduce the training toll required by the the NISQ era, aiming for both fast convergence to need fewer training cycles as traditional methods, as well as for reducing the impact OOV words have on a given model, thus lessening the training load on quantum systems by offloading it to classical machines.